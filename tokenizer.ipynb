{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "164030cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "POPPLER_PATH = os.getenv(\"POPPLER_PATH\")\n",
    "pytesseract.pytesseract.tesseract_cmd = os.getenv(\"PYTESSERACT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4d703be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, type, value, bbox):\n",
    "        self.type = type   # e.g., \"FIELD_LABEL\"\n",
    "        self.value = value # e.g., \"Name:\"\n",
    "        self.bbox = bbox   # (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92b8085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = convert_from_path(\n",
    "    \"./forms/forms.pdf\", \n",
    "    300,\n",
    "    poppler_path=POPPLER_PATH\n",
    ")\n",
    "\n",
    "data = pytesseract.image_to_data(pages[2], output_type=Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0caf48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ocr_data(data, gap_threshold=27):\n",
    "    \"\"\"\n",
    "    Groups Tesseract words into lines based on Block+Paragraph+Line ID,\n",
    "    then splits them horizontally if a large gap is detected.\n",
    "    \"\"\"\n",
    "    raw_lines = {}\n",
    "    n_boxes = len(data['text'])\n",
    "    \n",
    "    for i in range(n_boxes):\n",
    "        # Skip empty text or low confidence garbage\n",
    "        if int(data['conf'][i]) == -1 or not data['text'][i].strip():\n",
    "            continue\n",
    "        \n",
    "        # Creating an ID (key) for each word\n",
    "        line_id = f\"{data['block_num'][i]}_{data['par_num'][i]}_{data['line_num'][i]}\"\n",
    "        \n",
    "        word_info = {\n",
    "            \"text\": (data['text'][i].replace(\"_\", \"\")).strip(),\n",
    "            \"left\": data['left'][i],\n",
    "            \"top\": data['top'][i],\n",
    "            \"width\": data['width'][i],\n",
    "            \"right\": data['left'][i] + data['width'][i],\n",
    "            \"height\": data['height'][i]\n",
    "        }\n",
    "        \n",
    "        if line_id not in raw_lines:\n",
    "            raw_lines[line_id] = []\n",
    "        raw_lines[line_id].append(word_info)\n",
    "\n",
    "\n",
    "    # 2. Process each line: Merge words, but split on huge gaps\n",
    "    final_tokens = []\n",
    "\n",
    "    for line_id, words in raw_lines.items():\n",
    "        # Ensure words are sorted left-to-right (Tesseract usually does this, but be safe)\n",
    "        words.sort(key=lambda w: w['left'])\n",
    "        \n",
    "        # Initialize the first phrase with the first word\n",
    "        current_phrase_text = words[0]['text']\n",
    "        current_phrase_bbox = [words[0]['left'], words[0]['top'], words[0]['width'], words[0]['height']]\n",
    "        last_word_right = words[0]['right']\n",
    "\n",
    "        for i in range(1, len(words)):\n",
    "            word = words[i]\n",
    "            \n",
    "            # Calculate the gap between the end of the last word and start of this one\n",
    "            gap = word['left'] - last_word_right\n",
    "            \n",
    "            # DECISION: Is this a new field on the same line?\n",
    "            if gap > gap_threshold:\n",
    "                # YES: The gap is huge (e.g., between \"Name:\" and \"Phone:\")\n",
    "                # 1. Save the previous phrase as a complete token\n",
    "                \n",
    "                token_type = \"\"\n",
    "                token_id=0\n",
    "\n",
    "                if (current_phrase_text.strip().endswith(\":\")):\n",
    "                    token_type = \"FIELD_LABEL\"\n",
    "                    token_id = 3\n",
    "                else:\n",
    "                    token_type = \"NOTE\"\n",
    "                    token_id = 5\n",
    "                  \n",
    "                final_tokens.append({\n",
    "                    \"id\": token_id,\n",
    "                    \"type\": token_type,\n",
    "                    \"value\": current_phrase_text,\n",
    "                    \"x\": current_phrase_bbox[0],\n",
    "                    \"y\": current_phrase_bbox[1],\n",
    "                    \"w\": current_phrase_bbox[2],\n",
    "                    \"h\": current_phrase_bbox[3]\n",
    "                })\n",
    "                \n",
    "                # 2. Start a completely new phrase for the current word\n",
    "                current_phrase_text = word['text']\n",
    "                # Reset bbox to this new word's geometry\n",
    "                current_phrase_bbox = [word['left'], word['top'], word['width'], word['height']]\n",
    "                \n",
    "            else:\n",
    "                # NO: The gap is small. It's part of the same sentence.\n",
    "                current_phrase_text += \" \" + word['text']\n",
    "                \n",
    "                # Expand the width of the current phrase to include this word\n",
    "                # New Width = (Word Right Edge) - (Phrase Left Edge)\n",
    "                current_phrase_bbox[2] = word['right'] - current_phrase_bbox[0]\n",
    "                \n",
    "                # Update height to be the max height seen (optional, but good for bounding boxes)\n",
    "                current_phrase_bbox[3] = max(current_phrase_bbox[3], word['height'])\n",
    "            \n",
    "            # Update tracker for the next iteration\n",
    "            last_word_right = word['right']\n",
    "\n",
    "\n",
    "        # Append the final phrase of the line after the loop finishes\n",
    "        token_type = \"\"\n",
    "        token_id = 0\n",
    "\n",
    "        if (current_phrase_text.strip().endswith(\":\")):\n",
    "            token_type = \"FIELD_LABEL\"\n",
    "            token_id = 3\n",
    "        else:\n",
    "            token_type = \"NOTE\"\n",
    "            token_id = 5\n",
    "            \n",
    "        final_tokens.append({\n",
    "            \"id\": token_id,\n",
    "            \"type\": token_type,\n",
    "            \"value\": current_phrase_text,\n",
    "            \"x\": current_phrase_bbox[0],\n",
    "            \"y\": current_phrase_bbox[1],\n",
    "            \"w\": current_phrase_bbox[2],\n",
    "            \"h\": current_phrase_bbox[3]\n",
    "        })\n",
    "\n",
    "\n",
    "    # clean the final tokens\n",
    "    notes = [t for t in final_tokens if t[\"type\"] == \"NOTE\"]\n",
    "    heights = np.array([t[\"h\"] for t in notes])\n",
    "    median_h = np.median(heights)\n",
    "    max_h = np.max(heights)\n",
    "\n",
    "    img = cv2.imread(\"medical_form.jpg\")\n",
    "    page_height, page_width = img.shape[:2]\n",
    "\n",
    "    PAGE_WIDTH = page_width\n",
    "\n",
    "    def is_form_title(t, median_h):\n",
    "        return (\n",
    "            t[\"h\"] >= median_h * 1.3 and   # ← lower\n",
    "            t[\"y\"] < 300 and\n",
    "            t[\"w\"] > 0.5 * PAGE_WIDTH      # ← lower\n",
    "        )\n",
    "\n",
    "    \n",
    "    def is_section_title(t, median_h):\n",
    "        return (\n",
    "            t[\"h\"] >= median_h * 0.80 and          # similar to notes\n",
    "            t[\"w\"] >= 0.10 * PAGE_WIDTH and         # wider than labels\n",
    "            len(t[\"value\"].split()) <= 6 and\n",
    "            not t[\"value\"].strip().endswith(\":\")   # exclude labels\n",
    "        )\n",
    "\n",
    "\n",
    "    for t in final_tokens:\n",
    "        if t[\"type\"] != \"NOTE\":\n",
    "            continue\n",
    "\n",
    "        if is_form_title(t, median_h):\n",
    "            t[\"type\"] = \"FORM_TITLE\"\n",
    "            t[\"id\"] = 1\n",
    "        elif is_section_title(t, median_h):\n",
    "            t[\"type\"] = \"SECTION_TITLE\"\n",
    "            t[\"id\"] = 2\n",
    "        else:\n",
    "            t[\"type\"] = \"NOTE\"\n",
    "\n",
    "    \n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e37520b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'type': 'FORM_TITLE', 'value': 'Medical Consent Form (ABC Medics)', 'x': 153, 'y': 168, 'w': 1344, 'h': 79}\n",
      "{'id': 2, 'type': 'SECTION_TITLE', 'value': 'Patient Information', 'x': 155, 'y': 391, 'w': 575, 'h': 45}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Full Name:', 'x': 154, 'y': 508, 'w': 226, 'h': 33}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Date of Birth:', 'x': 1539, 'y': 508, 'w': 273, 'h': 33}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Address:', 'x': 150, 'y': 707, 'w': 185, 'h': 33}\n",
      "{'id': 2, 'type': 'SECTION_TITLE', 'value': 'Emergency Contact', 'x': 155, 'y': 1108, 'w': 587, 'h': 56}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Emergency Contact Name:', 'x': 154, 'y': 1224, 'w': 570, 'h': 42}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Phone Number:', 'x': 1677, 'y': 1224, 'w': 330, 'h': 33}\n",
      "{'id': 2, 'type': 'SECTION_TITLE', 'value': 'Medical Details', 'x': 155, 'y': 1427, 'w': 462, 'h': 45}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Medical Provider Name:', 'x': 154, 'y': 1544, 'w': 505, 'h': 33}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Treatment Description:', 'x': 150, 'y': 1644, 'w': 489, 'h': 42}\n",
      "{'id': 2, 'type': 'SECTION_TITLE', 'value': 'Signatures', 'x': 152, 'y': 2265, 'w': 318, 'h': 56}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Patient Signature:', 'x': 154, 'y': 2382, 'w': 371, 'h': 42}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Date:', 'x': 1605, 'y': 2382, 'w': 108, 'h': 33}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Guardian Signature:', 'x': 153, 'y': 2481, 'w': 421, 'h': 42}\n",
      "{'id': 3, 'type': 'FIELD_LABEL', 'value': 'Date:', 'x': 1605, 'y': 2481, 'w': 108, 'h': 33}\n"
     ]
    }
   ],
   "source": [
    "text_tokens = process_ocr_data(data)\n",
    "\n",
    "for obj in text_tokens:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2fb3e",
   "metadata": {},
   "source": [
    "## Open CV Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"medical_form.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f84f2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'FIELD_SPACE', 'value': '____', 'x': 403, 'y': 493, 'w': 1053, 'h': 59}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 1855, 'y': 497, 'w': 613, 'h': 61}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 410, 'y': 706, 'w': 2059, 'h': 233}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 768, 'y': 1211, 'w': 798, 'h': 59}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 2058, 'y': 1211, 'w': 430, 'h': 55}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 728, 'y': 1534, 'w': 798, 'h': 59}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 148, 'y': 1726, 'w': 2324, 'h': 354}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 1823, 'y': 2369, 'w': 574, 'h': 72}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 649, 'y': 2370, 'w': 798, 'h': 59}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 648, 'y': 2472, 'w': 797, 'h': 59}, {'type': 'FIELD_SPACE', 'value': '____', 'x': 1825, 'y': 2472, 'w': 574, 'h': 72}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# binary image (invert so lines are white)\n",
    "_, bw = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Detect horizontal lines\n",
    "h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n",
    "h_lines = cv2.morphologyEx(bw, cv2.MORPH_OPEN, h_kernel)\n",
    "\n",
    "# Detect vertical lines\n",
    "v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))\n",
    "v_lines = cv2.morphologyEx(bw, cv2.MORPH_OPEN, v_kernel)\n",
    "\n",
    "# Combine lines\n",
    "lines = cv2.add(h_lines, v_lines)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "visual_tokens = []\n",
    "for cnt in contours:\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    if w > 70 and h > 5:\n",
    "        # Store as a dictionary or object to match your Token style\n",
    "        visual_tokens.append({\n",
    "            'type': 'FIELD_SPACE', # or CHECKBOX depending on shape\n",
    "            'value': '____',\n",
    "            'x': x, 'y': y, 'w': w, 'h': h\n",
    "        })\n",
    "\n",
    "# CRITICAL STEP: SORT BY Y (Top-to-Bottom), THEN X (Left-to-Right)\n",
    "# We use y // 10 to allow for slight \"wobble\" in alignment (Row Clustering)\n",
    "visual_tokens.sort(key=lambda b: (b['y'] // 10, b['x']))\n",
    "\n",
    "print(visual_tokens)\n",
    "cv2.imwrite(\"detected_fields.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ff3aa",
   "metadata": {},
   "source": [
    "## Merging and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aad2d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_sort_tokens(text_tokens, visual_tokens, row_tolerance=20):\n",
    "    # 1. Unified List\n",
    "    all_tokens = text_tokens + visual_tokens\n",
    "\n",
    "    # 2. Initial Sort by Y (Top to Bottom)\n",
    "    # This gets them roughly in order so we can cluster them\n",
    "    all_tokens.sort(key=lambda t: t['y'])\n",
    "\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    \n",
    "    # We track the \"average Y\" of the current row to handle drift\n",
    "    current_row_y = 0 \n",
    "\n",
    "    if all_tokens:\n",
    "        current_row = [all_tokens[0]]\n",
    "        current_row_y = all_tokens[0]['y']\n",
    "\n",
    "    for i in range(1, len(all_tokens)):\n",
    "        token = all_tokens[i]\n",
    "        \n",
    "        # 3. Check Vertical Distance\n",
    "        # If this token is within 'row_tolerance' pixels of the current row's Y...\n",
    "        if abs(token['y'] - current_row_y) <= row_tolerance:\n",
    "            current_row.append(token)\n",
    "            \n",
    "            # Optional: Update average Y (moving average) to follow the line's drift\n",
    "            # current_row_y = (current_row_y + token['y']) / 2\n",
    "        else:\n",
    "            # It's a new line! \n",
    "            # a. Sort the OLD row by X (Left to Right)\n",
    "            current_row.sort(key=lambda t: t['x'])\n",
    "            rows.append(current_row)\n",
    "            \n",
    "            # b. Start the NEW row\n",
    "            current_row = [token]\n",
    "            current_row_y = token['y']\n",
    "\n",
    "    # Don't forget the last row\n",
    "    if current_row:\n",
    "        current_row.sort(key=lambda t: t['x'])\n",
    "        rows.append(current_row)\n",
    "\n",
    "    # 4. Flatten into a single stream\n",
    "    # This turns [[Row1_Item1, Row1_Item2], [Row2_Item1]] into [Item1, Item2, Item1...]\n",
    "    final_stream = [token for row in rows for token in row]\n",
    "    \n",
    "    return final_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1aa44952",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = merge_and_sort_tokens(text_tokens, visual_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "16dfc09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'TEXT_FRAGMENT', 'value': 'Medical Consent Form (ABC Medics)', 'x': 153, 'y': 168, 'w': 1344, 'h': 79}\n",
      "{'type': 'TEXT_FRAGMENT', 'value': 'Patient Information', 'x': 155, 'y': 391, 'w': 575, 'h': 45}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Full Name:', 'x': 154, 'y': 508, 'w': 226, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 403, 'y': 493, 'w': 1053, 'h': 59}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Date of Birth:', 'x': 1539, 'y': 508, 'w': 273, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 1855, 'y': 497, 'w': 613, 'h': 61}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Address:', 'x': 150, 'y': 707, 'w': 185, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 410, 'y': 706, 'w': 2059, 'h': 233}\n",
      "{'type': 'TEXT_FRAGMENT', 'value': 'Emergency Contact', 'x': 155, 'y': 1108, 'w': 587, 'h': 56}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Emergency Contact Name:', 'x': 154, 'y': 1224, 'w': 570, 'h': 42}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 768, 'y': 1211, 'w': 798, 'h': 59}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Phone Number:', 'x': 1677, 'y': 1224, 'w': 330, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 2058, 'y': 1211, 'w': 430, 'h': 55}\n",
      "{'type': 'TEXT_FRAGMENT', 'value': 'Medical Details', 'x': 155, 'y': 1427, 'w': 462, 'h': 45}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Medical Provider Name:', 'x': 154, 'y': 1544, 'w': 505, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 728, 'y': 1534, 'w': 798, 'h': 59}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Treatment Description:', 'x': 150, 'y': 1644, 'w': 489, 'h': 42}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 148, 'y': 1726, 'w': 2324, 'h': 354}\n",
      "{'type': 'TEXT_FRAGMENT', 'value': 'Signatures', 'x': 152, 'y': 2265, 'w': 318, 'h': 56}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Patient Signature:', 'x': 154, 'y': 2382, 'w': 371, 'h': 42}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 649, 'y': 2370, 'w': 798, 'h': 59}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Date:', 'x': 1605, 'y': 2382, 'w': 108, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 1823, 'y': 2369, 'w': 574, 'h': 72}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Guardian Signature:', 'x': 153, 'y': 2481, 'w': 421, 'h': 42}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 648, 'y': 2472, 'w': 797, 'h': 59}\n",
      "{'type': 'FIELD_LABEL', 'value': 'Date:', 'x': 1605, 'y': 2481, 'w': 108, 'h': 33}\n",
      "{'type': 'FIELD_SPACE', 'value': '____', 'x': 1825, 'y': 2472, 'w': 574, 'h': 72}\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b8f3afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Iterate through your token stream\n",
    "for t in tokens:\n",
    "    x, y, w, h = t['x'], t['y'], t['w'], t['h']\n",
    "    \n",
    "    # COLOR CODING SCHEME (BGR Format)\n",
    "    if t['type'] == 'FIELD_SPACE':\n",
    "        color = (0, 0, 255)      # Red for Input Boxes\n",
    "        thickness = 2\n",
    "    elif t['type'] == 'CHECKBOX':\n",
    "        color = (0, 0, 255)      # Red for Checkboxes\n",
    "        thickness = 2\n",
    "    elif t['type'] == 'FIELD_LABEL':\n",
    "        color = (0, 255, 0)      # Green for Labels\n",
    "        thickness = 2\n",
    "    elif t['type'] == 'SECTION_TITLE':\n",
    "        color = (255, 0, 0)      # Blue for Titles\n",
    "        thickness = 3\n",
    "    else:\n",
    "        color = (255, 255, 0)    # Cyan for Fragments/Notes\n",
    "        thickness = 1\n",
    "\n",
    "    # 3. Draw the Rectangle\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), color, thickness)\n",
    "    \n",
    "    # 4. Draw the Label (Tiny text above the box)\n",
    "    # This helps you see if the SYSTEM thinks it's a \"Label\" or a \"Note\"\n",
    "    label_text = f\"{t['type']} ({t['value'][:10]}...)\"\n",
    "    cv2.putText(img, label_text, (x, y - 5), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "\n",
    "# 5. Save the result\n",
    "cv2.imwrite(\"final_detected.jpg\", img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9de6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
